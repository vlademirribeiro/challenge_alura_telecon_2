{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMX8M8TkvuOqeP+h+ny3ypq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlademirribeiro/challenge_alura_telecon_2/blob/main/telecom2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desafio Telecom X - Parte 2: Modelagem Preditiva de Churn"
      ],
      "metadata": {
        "id": "CnpG5219xWgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prop√≥sito da An√°lise"
      ],
      "metadata": {
        "id": "aeurTg19xlQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook √© a resposta ao desafio da Telecom X para a equipe de Machine Learning. O objetivo principal √© desenvolver um modelo preditivo capaz de identificar clientes com alta probabilidade de cancelar seus servi√ßos (churn). A an√°lise se baseia em um conjunto de dados pr√©-processado, e o resultado final ser√° um modelo treinado e avaliado, acompanhado de insights estrat√©gicos para a empresa."
      ],
      "metadata": {
        "id": "XGnEuc9uxxq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roteiro do Projeto"
      ],
      "metadata": {
        "id": "MVVniVtwx8tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O desenvolvimento deste projeto seguir√° as seguintes etapas:\n",
        "\n",
        "* Configura√ß√£o e Carregamento dos Dados: Prepara√ß√£o do ambiente de trabalho e importa√ß√£o das bibliotecas e do conjunto de dados.\n",
        "* An√°lise Explorat√≥ria dos Dados (EDA): Investiga√ß√£o inicial dos dados para entender a distribui√ß√£o das vari√°veis, identificar padr√µes e formular hip√≥teses.\n",
        "* Pr√©-processamento e Prepara√ß√£o dos Dados: Tratamento das vari√°veis para que possam ser utilizadas pelos algoritmos de machine learning (encoding, normaliza√ß√£o, etc.).\n",
        "* Modelagem de Machine Learning: Treinamento de m√∫ltiplos algoritmos de classifica√ß√£o para prever o churn.\n",
        "* Avalia√ß√£o dos Modelos: Utiliza√ß√£o de m√©tricas de performance para escolher o modelo mais eficaz.\n",
        "* An√°lise de Resultados e Conclus√µes Estrat√©gicas: Interpreta√ß√£o do modelo final para extrair insights de neg√≥cio e apresentar recomenda√ß√µes.\n"
      ],
      "metadata": {
        "id": "HE3N2AZIyBqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Configura√ß√£o e Carregamento dos Dados\n",
        "Nesta primeira etapa, vamos importar as bibliotecas essenciais para o projeto e carregar nosso conjunto de dados. A base de dados, no formato JSON, ser√° carregada diretamente do reposit√≥rio GitHub do projeto, garantindo a reprodutibilidade da an√°lise."
      ],
      "metadata": {
        "id": "EXLNPqGSzSlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "1b00InxWnf5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas do Skit Learn\n",
        "from sklearn.model_selection import train_test_split #Utilizada para separar dados pra treino e teste\n",
        "from sklearn.preprocessing import StandardScaler #Utilizada para fazer a padroniza√ß√£o dos dados\n",
        "from sklearn.preprocessing import LabelEncoder #Utilizada para fazer o OneHotEncoding\n",
        "from sklearn.metrics import accuracy_score #Utilizada para avaliar a acur√°cia do modelo preditivo\n",
        "from sklearn.neighbors import KNeighborsClassifier #Nosso Algoritmo para cria√ß√£o do modelo\n",
        "from imblearn import under_sampling, over_sampling #Utilizada para fazer o balanceamento de dados\n",
        "from imblearn.over_sampling import SMOTE #Utilizada para fazer o balanceamento de dados\n",
        "from sklearn.preprocessing import StandardScaler # Utilizado para fazer a normaliza√ß√£o dos dados\n",
        "from sklearn.preprocessing import MinMaxScaler # Utilizado para fazer a normaliza√ß√£o dos dados\n",
        "from sklearn.preprocessing import LabelEncoder # Utilizado para fazer o OneHotEncoding\n",
        "from sklearn.linear_model import LinearRegression # Algoritmo de Regress√£o Linear\n",
        "from sklearn.metrics import r2_score # Utilizado para medir a acuracia do modelo preditivo\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "metadata": {
        "id": "ZfxUKPRW08v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/ingridcristh/challenge2-data-science/refs/heads/main/TelecomX_Data.json\"\n",
        "response = requests.get(url)\n",
        "data = response.json()\n"
      ],
      "metadata": {
        "id": "foqXGyI2nf9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertendo para dataframe\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HHT5qbZIngAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados = pd.json_normalize(data)\n",
        "dados_normalizados.head()"
      ],
      "metadata": {
        "id": "HxCapwHXngCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.to_csv(\"TelecomX_Data.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"CSV criado com sucesso!\")"
      ],
      "metadata": {
        "id": "RIqMuovf3KWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.shape"
      ],
      "metadata": {
        "id": "g5Nbmtr2ngEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.info()"
      ],
      "metadata": {
        "id": "bo8R9wlGngGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dados_normalizados.columns.tolist())"
      ],
      "metadata": {
        "id": "4vQeUQPrqZe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.drop(['customerID'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "TIG0oN85ngJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_fix = [\n",
        "    'internet.OnlineSecurity', 'internet.OnlineBackup', 'internet.DeviceProtection',\n",
        "    'internet.TechSupport', 'internet.StreamingTV', 'internet.StreamingMovies'\n",
        "]\n",
        "\n",
        "for col in cols_to_fix:\n",
        "    dados_normalizados[col] = dados_normalizados[col].replace('No internet service', 'No')"
      ],
      "metadata": {
        "id": "mjMLM-gRngMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.head()"
      ],
      "metadata": {
        "id": "6uSwTKZntiT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.isna().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iJ68WmcGtiRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.describe()\n"
      ],
      "metadata": {
        "id": "WhfFs5H0tiPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.nunique()"
      ],
      "metadata": {
        "id": "DxbDxUShtiNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variaveis_numericas = []\n",
        "\n",
        "for i in dados_normalizados.columns[0:22].tolist():\n",
        "\n",
        "    # Verifica se o tipo de dado da coluna atual √© 'int64' ou 'float64' (n√∫meros inteiros ou reais)\n",
        "    if dados_normalizados.dtypes[i] == 'int64' or dados_normalizados.dtypes[i] == 'float64':\n",
        "\n",
        "        # Imprime o nome da coluna e o tipo de dado dela\n",
        "        print(i, ':', dados_normalizados.dtypes[i])\n",
        "\n",
        "        # Adiciona o nome da coluna √† lista de vari√°veis num√©ricas\n",
        "        variaveis_numericas.append(i)"
      ],
      "metadata": {
        "id": "H4rFxROvtiJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho da figura em polegadas: largura = 14, altura = 5\n",
        "plt.rcParams[\"figure.figsize\"] = [14.00, 5.00]\n",
        "\n",
        "# Garante que os elementos do gr√°fico se ajustem automaticamente ao layout\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "\n",
        "# Como temos 5 vari√°veis, podemos criar uma grade de 1 linha e 5 colunas\n",
        "f, axes = plt.subplots(1, 3)  # 1 linha e 5 colunas\n",
        "\n",
        "# Inicializa a posi√ß√£o dos gr√°ficos\n",
        "coluna = 0\n",
        "\n",
        "# Percorre todas as vari√°veis num√©ricas selecionadas\n",
        "for i in variaveis_numericas[:5]:  # Seleciona as primeiras 5 vari√°veis num√©ricas\n",
        "    # Cria um boxplot para a vari√°vel i na posi√ß√£o [0][coluna] da grade\n",
        "    sns.boxplot(data=dados_normalizados, y=i, ax=axes[coluna])\n",
        "\n",
        "    # Avan√ßa para a pr√≥xima coluna\n",
        "    coluna += 1\n",
        "\n",
        "# Exibe todos os gr√°ficos gerados\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6xNC-OkJtiFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para armazenar as vari√°veis categ√≥ricas\n",
        "variaveis_categoricas = []\n",
        "\n",
        "# Itera sobre as colunas de 0 at√© a 47 (isso seleciona as primeiras 48 colunas)\n",
        "for i in dados_normalizados.columns[0:23]:  # Corrigindo a itera√ß√£o para cada coluna\n",
        "    if dados_normalizados.dtypes[i] == 'object' or dados_normalizados.dtypes[i] == 'category':\n",
        "        # Imprime o nome da vari√°vel e o seu tipo\n",
        "        print(i, ':', dados_normalizados.dtypes[i])\n",
        "        # Adiciona a vari√°vel √† lista de vari√°veis categ√≥ricas\n",
        "        variaveis_categoricas.append(i)\n",
        "\n",
        "\n",
        "# Exibe as vari√°veis categ√≥ricas identificadas\n",
        "print(\"Vari√°veis categ√≥ricas:\",len(variaveis_categoricas), variaveis_categoricas)"
      ],
      "metadata": {
        "id": "FHjvLTQEtiCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "def plot_categoricas(dados, variaveis, ncols=4, figsize=(15, 15)):\n",
        "    \"\"\"\n",
        "    Plota gr√°ficos de contagem para vari√°veis categ√≥ricas.\n",
        "\n",
        "    :param dados: DataFrame com os dados normalizados\n",
        "    :param variaveis: lista de colunas categ√≥ricas\n",
        "    :param ncols: n√∫mero de colunas na grade de gr√°ficos\n",
        "    :param figsize: tamanho da figura\n",
        "    \"\"\"\n",
        "    n_plots = len(variaveis)\n",
        "    nrows = math.ceil(n_plots / ncols)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "    axes = axes.flatten()  # transforma em lista 1D para simplificar\n",
        "\n",
        "    for ax, var in zip(axes, variaveis):\n",
        "        sns.countplot(data=dados, x=var, ax=ax)\n",
        "        ax.set_title(var)\n",
        "\n",
        "    # Remove eixos extras (se sobrar espa√ßo na grade)\n",
        "    for ax in axes[n_plots:]:\n",
        "        ax.remove()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_hWtxOJvth_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = [15.00, 5.00]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "\n",
        "# Aqui definimos em quantas linhas e colunas queremos exibir os gr√°ficos\n",
        "f, axes = plt.subplots(2, 3) #4 linhas e 3 colunas\n",
        "\n",
        "linha = 0\n",
        "coluna = 0\n",
        "\n",
        "for i in variaveis_numericas:\n",
        "    sns.histplot(data = dados_normalizados, x=i, ax=axes[linha][coluna])\n",
        "    coluna += 1\n",
        "    if coluna == 3:\n",
        "        linha += 1\n",
        "        coluna = 0\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PYI4Dputth9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.head()"
      ],
      "metadata": {
        "id": "4RGoytI8th63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variaveis_categoricas"
      ],
      "metadata": {
        "id": "d6e3xSoHzHyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapa_binario = {'Yes': 1, 'No': 0, 'Female': 0, 'Male': 1}\n",
        "colunas_para_mapear = ['Churn', 'customer.gender', 'customer.Partner', 'customer.Dependents', 'phone.PhoneService', 'account.PaperlessBilling']\n",
        "\n",
        "for col in colunas_para_mapear:\n",
        "    # Apenas mapeia se a coluna ainda for do tipo 'object'\n",
        "    if dados_normalizados[col].dtype == 'object':\n",
        "        dados_normalizados[col] = dados_normalizados[col].map(mapa_binario)\n",
        "\n",
        "# Identifica colunas multinomiais que ainda s√£o 'object'\n",
        "colunas_multinomiais = dados_normalizados.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Aplica One-Hot Encoding (get_dummies)\n",
        "dados = pd.get_dummies(dados_normalizados, columns=colunas_multinomiais, drop_first=True)\n",
        "\n",
        "print(\"Dados ap√≥s One-Hot Encoding:\")\n",
        "dados.head()"
      ],
      "metadata": {
        "id": "GBdq1xGLzHvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos aplicar a normaliza√ß√£o\n",
        "\n",
        "# Selecionando apenas colunas num√©ricas (exceto a vari√°vel target, se j√° estiver separada)\n",
        "colunas_numericas = dados_normalizados.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Inicializando o scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Aplicando a normaliza√ß√£o\n",
        "dados_normalizados[colunas_numericas] = scaler.fit_transform(dados[colunas_numericas])\n",
        "\n",
        "# Exibindo os dados normalizados\n",
        "dados.head()\n"
      ],
      "metadata": {
        "id": "UbTpxTQBzHsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = dados_normalizados.corr(numeric_only=True)\n",
        "\n",
        "# Cria uma m√°scara para ocultar a parte superior da matriz (duplicada)\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Configura√ß√£o da figura\n",
        "fig, ax = plt.subplots(figsize=(8, 10))\n",
        "\n",
        "# Heatmap estilizado\n",
        "sns.heatmap(\n",
        "    np.round(corr, 2),\n",
        "    mask=mask,               # M√°scara triangular\n",
        "    cmap='RdBu',             # Mapa de cores contrastante\n",
        "    vmax=1, vmin=-1, center=0,\n",
        "    square=True,\n",
        "    linewidths=.5,\n",
        "    annot=True,              # Mostrar valores\n",
        "    annot_kws={\"size\": 10},   # Tamanho do texto\n",
        "    cbar_kws={\"shrink\": .5}   # Barra de cores menor\n",
        ")\n",
        "\n",
        "# Ajusta r√≥tulos\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
        "plt.yticks(rotation=0, fontsize=12)\n",
        "\n",
        "# T√≠tulo\n",
        "plt.title('Matriz de Correla√ß√£o das Vari√°veis', fontsize=18, pad=20)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ny2A9JPfzHoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 10))\n",
        "\n",
        "# Calcula a matriz de correla√ß√£o do DataFrame e filtra para mostrar apenas as correla√ß√µes com 'Churn'\n",
        "# Ordena os valores para visualizar facilmente os fatores mais correlacionados (positiva ou negativamente)\n",
        "heatmap = sns.heatmap(\n",
        "    dados_normalizados.corr(numeric_only=True)[['Churn']].sort_values(by='Churn', ascending=False),\n",
        "    vmin=-1, vmax=1, annot=True, cmap='BrBG'\n",
        ")\n",
        "\n",
        "# T√≠tulo do gr√°fico\n",
        "heatmap.set_title('Features Correlacionadas com Churn', fontdict={'fontsize': 18}, pad=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MDHAJGIkzHlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.info()"
      ],
      "metadata": {
        "id": "VtvlaCirzHix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_normalizados.head()\n"
      ],
      "metadata": {
        "id": "4NJ5Ap8DzHf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Separar X e y do dataframe original (antes do SMOTE)\n",
        "X = dados_normalizados.drop('Churn', axis=1)\n",
        "y = dados_normalizados['Churn']\n"
      ],
      "metadata": {
        "id": "YL_aY0d4zHcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Dividir em dados de treino e teste PRIMEIRO\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.3, random_state=40)"
      ],
      "metadata": {
        "id": "DAXffAFm7kgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibe a distribui√ß√£o normalizada da vari√°vel 'Churn' em percentual\n",
        "churn_distribution = dados_normalizados['Churn'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Exibe os resultados com duas casas decimais\n",
        "print(churn_distribution.apply(lambda x: f'{x:.2f}%'))"
      ],
      "metadata": {
        "id": "iV_-5yKPzHZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Quantidade de NaN em y:\", y.isna().sum())\n",
        "print(\"Total de linhas em y:\", len(y))\n",
        "print(\"Propor√ß√£o de NaN em y:\", y.isna().mean())\n",
        "\n",
        "print(\"\\nVerificando X:\")\n",
        "print(X.isna().sum())  # mostra colunas com NaN\n"
      ],
      "metadata": {
        "id": "ulhNKWYLBIaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape de X:\", X.shape)\n",
        "print(\"Shape de y:\", y.shape)\n",
        "\n",
        "# Se estiver usando treino/teste\n",
        "print(\"Shape de X_treino:\", X_treino.shape)\n",
        "print(\"Shape de y_treino:\", y_treino.shape)\n"
      ],
      "metadata": {
        "id": "Xl4Dqo88Bv6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibe a distribui√ß√£o normalizada da vari√°vel 'Churn' em percentual\n",
        "churn_distribution = dados['Churn'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Exibe os resultados com duas casas decimais\n",
        "print(churn_distribution.apply(lambda x: f'{x:.2f}%'))"
      ],
      "metadata": {
        "id": "gyI1eQIK5nR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Colunas num√©ricas ‚Üí preencher com mediana\n",
        "num_cols = X.select_dtypes(include='number').columns\n",
        "X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
        "\n",
        "# Colunas categ√≥ricas ‚Üí preencher com moda\n",
        "cat_cols = X.select_dtypes(include='object').columns\n",
        "for col in cat_cols:\n",
        "    X[col] = X[col].fillna(X[col].mode()[0])\n",
        "\n",
        "# Tratar NaN do alvo e garantir 0/1\n",
        "if y.dtype.kind in 'O':  # Se alvo for string\n",
        "    y = y.fillna(y.mode()[0])\n",
        "    if set(y.unique()) == {'Yes','No'}:\n",
        "        y = y.map({'No':0,'Yes':1})\n",
        "else:\n",
        "    y = y.fillna(y.mode()[0]).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "AgTyqYTn4aqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fun√ß√£o de valida√ß√£o\n",
        "# =========================\n",
        "def validar_dados(X, y, etapa=\"pr√©-processamento\"):\n",
        "    print(f\"\\nüîé Validando dados ap√≥s etapa: {etapa}\")\n",
        "    print(f\"‚û° Shape de X: {X.shape}\")\n",
        "    print(f\"‚û° Shape de y: {y.shape}\")\n",
        "\n",
        "    if X.shape[0] == 0 or y.shape[0] == 0:\n",
        "        raise ValueError(f\"‚ùå Dataset vazio ap√≥s {etapa}.\")\n",
        "\n",
        "    na_x = X.isna().sum()\n",
        "    if na_x.sum() > 0:\n",
        "        print(\"‚ö†Ô∏è Aten√ß√£o: X cont√©m NaN nas colunas abaixo:\")\n",
        "        print(na_x[na_x > 0])\n",
        "\n",
        "    if y.isna().sum() > 0:\n",
        "        print(f\"‚ö†Ô∏è Aten√ß√£o: y cont√©m {y.isna().sum()} NaN.\")\n",
        "\n",
        "    print(\"‚úÖ Dados v√°lidos!\")\n",
        "\n",
        "validar_dados(X, y, etapa=\"ap√≥s tratamento de NaN\")"
      ],
      "metadata": {
        "id": "JnXf1WWpAeNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5Ô∏è‚É£ Split treino/teste\n",
        "# =========================\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "validar_dados(X_treino, y_treino, etapa=\"train_test_split\")\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ One-Hot Encoding nas colunas categ√≥ricas\n",
        "# =========================\n",
        "X_treino_enc = pd.get_dummies(X_treino, drop_first=True)\n",
        "X_teste_enc  = pd.get_dummies(X_teste, drop_first=True)\n",
        "\n",
        "# Garantir que as colunas do teste sejam iguais √†s do treino\n",
        "X_teste_enc = X_teste_enc.reindex(columns=X_treino_enc.columns, fill_value=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "Oe93IFwR4akk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#‚É£‚É£ Distribui√ß√£o original do alvo\n",
        "# =========================\n",
        "print(\"\\nüîπ Distribui√ß√£o de Churn antes do SMOTE:\")\n",
        "print(y_treino.value_counts(normalize=True) * 100)"
      ],
      "metadata": {
        "id": "Ef5pNFax4aKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8Ô∏è‚É£ Aplicar SMOTE\n",
        "# =========================\n",
        "smote = SMOTE(random_state=42)\n",
        "X_treino_res, y_treino_res = smote.fit_resample(X_treino_enc, y_treino)"
      ],
      "metadata": {
        "id": "O2UM5iaV_n9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Distribui√ß√£o ap√≥s SMOTE\n",
        "# =========================\n",
        "print(\"\\nüîπ Distribui√ß√£o de Churn ap√≥s SMOTE:\")\n",
        "print(pd.Series(y_treino_res).value_counts(normalize=True) * 100)\n",
        "\n",
        "print(f\"\\n‚úÖ SMOTE aplicado! Novo shape treino: {X_treino_res.shape}\")"
      ],
      "metadata": {
        "id": "N3thWir6_n0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "kVals = range(3, 10, 2)\n",
        "acuracias = []\n",
        "start = time.time()\n",
        "\n",
        "for k in kVals:\n",
        "    modeloKNN = KNeighborsClassifier(n_neighbors=k)\n",
        "    modeloKNN.fit(X_treino_res, y_treino_res)\n",
        "\n",
        "    score = modeloKNN.score(X_teste_enc, y_teste)  # usa teste codificado!\n",
        "    print(f\"Com valor de k = {k}, a acur√°cia √© = {score * 100:.2f}%\")\n",
        "    acuracias.append(score)\n",
        "\n",
        "# Melhor valor de k\n",
        "i = np.argmax(acuracias)\n",
        "print(f\"\\nüî• O valor de k = {kVals[i]} alcan√ßou a mais alta acur√°cia de {acuracias[i] * 100:.2f}% nos dados de valida√ß√£o!\")\n"
      ],
      "metadata": {
        "id": "z5ZLcJp-_nbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lr = LogisticRegression(random_state=42, max_iter=1000)  # aumento de itera√ß√µes p/ evitar warning\n",
        "lr.fit(X_treino_res, y_treino_res)\n",
        "\n",
        "\n",
        "y_pred_lr = lr.predict(X_teste_enc)             # usa teste codificado\n",
        "y_prob_lr = lr.predict_proba(X_teste_enc)[:, 1] # probabilidades para curva ROC\n",
        "\n",
        "\n",
        "print(\"=== üìä Regress√£o Log√≠stica ===\")\n",
        "print(\"Acur√°cia:\", round(accuracy_score(y_teste, y_pred_lr), 4))\n",
        "print(\"ROC AUC:\", round(roc_auc_score(y_teste, y_prob_lr), 4))\n",
        "print(\"\\nMatriz de Confus√£o:\\n\", confusion_matrix(y_teste, y_pred_lr))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_teste, y_pred_lr))\n"
      ],
      "metadata": {
        "id": "vuAPsMSKDO41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# =========================\n",
        "# 2Ô∏è‚É£ Grade de par√¢metros\n",
        "# =========================\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],       # n√∫mero de √°rvores\n",
        "    'max_depth': [10, 20],                 # profundidade m√°xima da √°rvore\n",
        "    'min_samples_split': [2, 5, 10],       # m√≠nimo de amostras para dividir um n√≥\n",
        "    'min_samples_leaf': [1, 2, 4],         # m√≠nimo de amostras por folha\n",
        "    'max_features': ['sqrt', 'log2']       # como selecionar features em cada divis√£o\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 3Ô∏è‚É£ Busca em grade (GridSearchCV)\n",
        "# =========================\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                     # 5-fold cross validation\n",
        "    scoring='roc_auc',        # m√©trica alvo\n",
        "    n_jobs=-1,                # usar todos os n√∫cleos dispon√≠veis\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4Ô∏è‚É£ Treinamento\n",
        "# =========================\n",
        "grid_search.fit(X_treino_res, y_treino_res)\n",
        "\n",
        "print(\"\\n=== üå≥ Random Forest com GridSearchCV ===\")\n",
        "print(\"Melhores par√¢metros encontrados:\", grid_search.best_params_)\n",
        "print(\"Melhor pontua√ß√£o ROC AUC (valida√ß√£o cruzada):\", round(grid_search.best_score_, 4))\n",
        "\n",
        "# =========================\n",
        "# 5Ô∏è‚É£ Avalia√ß√£o final no conjunto de teste\n",
        "# =========================\n",
        "best_rf = grid_search.best_estimator_  # melhor modelo encontrado\n",
        "\n",
        "# Previs√µes no conjunto de teste\n",
        "y_pred_rf = best_rf.predict(X_teste_enc)\n",
        "y_prob_rf = best_rf.predict_proba(X_teste_enc)[:, 1]\n",
        "\n",
        "print(\"\\nAvalia√ß√£o no conjunto de teste:\")\n",
        "print(\"Acur√°cia:\", round(accuracy_score(y_teste, y_pred_rf), 4))\n",
        "print(\"ROC AUC:\", round(roc_auc_score(y_teste, y_prob_rf), 4))\n",
        "print(\"\\nMatriz de Confus√£o:\\n\", confusion_matrix(y_teste, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_teste, y_pred_rf))"
      ],
      "metadata": {
        "id": "PpbI5UHwEMoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5Ô∏è‚É£ Avalia√ß√£o final no conjunto de teste\n",
        "# =========================\n",
        "best_rf = grid_search.best_estimator_  # melhor modelo encontrado\n",
        "\n",
        "# Previs√µes no conjunto de teste\n",
        "y_pred_rf = best_rf.predict(X_teste_enc)\n",
        "y_prob_rf = best_rf.predict_proba(X_teste_enc)[:, 1]\n",
        "\n",
        "print(\"\\nAvalia√ß√£o no conjunto de teste:\")\n",
        "print(\"Acur√°cia:\", round(accuracy_score(y_teste, y_pred_rf), 4))\n",
        "print(\"ROC AUC:\", round(roc_auc_score(y_teste, y_prob_rf), 4))\n",
        "print(\"\\nMatriz de Confus√£o:\\n\", confusion_matrix(y_teste, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_teste, y_pred_rf))\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ Resumo do melhor modelo\n",
        "# =========================\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\" üéØ MELHOR MODELO PARA ESTA AN√ÅLISE üéØ \".center(60, \" \"))\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüìå Modelo escolhido: RandomForestClassifier\")\n",
        "print(f\"üîπ Melhores par√¢metros: {best_params}\")\n",
        "print(f\"üìä Melhor ROC AUC: {best_score:.4f}\")\n",
        "print(\"\\n‚úÖ Este modelo apresentou a melhor performance entre os testados e ser√° utilizado para as pr√≥ximas etapas da an√°lise.\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "wcGSA-ykHJq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}